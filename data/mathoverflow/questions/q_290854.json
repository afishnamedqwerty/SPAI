{
  "id": "q_290854",
  "mathoverflow_id": 290854,
  "url": "https://mathoverflow.net/questions/290854/optimal-conditional-density-choice",
  "title": "pr.probability - Optimal conditional density choice",
  "body": "Suppose $X$ is a random variable whose density is denoted by $p(x)$ . We can observe a realization of $X$ and we want to take an action $Y$ to minimize $$E[(Y-X)^{2}]$$ What is the optimal choice of the conditional density $q(y|x)$ of $Y|X$ Formally: $$\\min_{q}\\{E[(Y-X)^{2}]=\\int(y-x)^{2}q(y|x)p(x)dydx\\}$$ such that $$(\\forall x) \\int q(y|x)dy=1 \\quad (1)$$ $$-E[E[\\log_{2}q(Y|X)|X]]-\\{-E[\\log_{2}(\\int q(Y|x)p(x)dx)]\\} \\leq C \\quad (2)$$ where $$-\\int E[\\log_{2}q(Y|X)|X]p(x)dx=-\\int\\int(\\int q(y|x)p(x)dx)p(x)\\log_{2}q(y|x)dxdy$$ $$-E[\\log_{2}(\\int q(Y|x)p(x)dx)]=\\int(\\int q(y|x)p(x)dx)\\log_{2}(\\int q(y|x)p(x)dx)dy$$ The intuition of the last constraint is based on information-theoretic entropy. Before observing $X$ , the entropy of $(X,Y)$ is defined as: $$-E[\\log_{2}(\\int q(Y|x)p(x)dx)]$$ After observing $X=x$ , the entropy is: $$-E[\\log_{2}q(Y|X)|X=x]$$ So the reduction in the entropy is: $$-E[\\log_{2}q(Y|X)|X=x]-\\{-E[\\log_{2}(\\int q(Y|x)p(x)dx)]\\}$$ The average reduction in the entropy is: $$-E[E[\\log_{2}q(Y|X)|X]]-\\{-E[\\log_{2}(\\int q(Y|x)p(x)dx)]\\}$$ Then there is an upper bound of the entropy reduction: $$-E[E[\\log_{2}q(Y|X)|X]]-\\{-E[\\log_{2}(\\int q(Y|x)p(x)dx)]\\} \\leq C$$ The trivial case is $C=\\infty$ For simplification, let $$h(y)=\\int q(y|x)p(x)dx$$ represent the density of $Y$ , then above constraint (2) can be translated as: $$-\\int\\int h(y)p(x)\\log_{2}q(y|x)dxdy-[-\\int h(y)\\log_{2}h(y)dy]\\leq C \\quad (2*)$$ The question is: When $X$ distribution is Gaussian, what is the optimal $q(y|x)$ given the constraint (1) and (2*)? A natural guess is that $q(y|x)$ is also Gaussian, how to show it? pr.probability oc.optimization-and-control calculus-of-variations Share Cite Improve this question Follow edited Jun 15, 2020 at 7:27 Community Bot 1 2 2 silver badges 3 3 bronze badges asked Jan 16, 2018 at 9:31 Galor 121 7 7 bronze badges $\\endgroup$ 4 1 $\\begingroup$ why not just identify $Y$ and $X$, so joint distribution $P(x,y)=p(x)\\delta(x-y)$ and $q(y|x)=\\delta(y-x)$; that would give you $E[(Y-X)^2]=0$, and you can't do better than that... $\\endgroup$ Carlo Beenakker –  Carlo Beenakker 2018-01-16 12:20:28 +00:00 Commented Jan 16, 2018 at 12:20 $\\begingroup$ @Carlo Beenakker, it seems that this construction can not satisfy last constraint except for the extreme case where C is infinite, how to deal with the constraint is the difficult and non-trivial point. $\\endgroup$ Galor –  Galor 2018-01-16 22:10:49 +00:00 Commented Jan 16, 2018 at 22:10 1 $\\begingroup$ I find it hard to read expressions like $E[\\log_2q(Y|X)|X]$. Can you restate the problem in pure integral terms, without any reference to conditionality? E.g.: given $p(x)$, find $q(x,y)$ which minimizes $\\int (x-y)^2 q(x,y) dx dy$ subject to $\\int q(x,y)dy = p(x)$ and ... what? $\\endgroup$ user44143 –  user44143 2018-01-17 00:54:35 +00:00 Commented Jan 17, 2018 at 0:54 1 $\\begingroup$ @MattF. I add a constraint (2*) and (2*) is an equivalent translation from (2) and the expectation operator is represented by the integral sign. $\\endgroup$ Galor –  Galor 2018-01-18 09:32:42 +00:00 Commented Jan 18, 2018 at 9:32 Add a comment  |  0 Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) You must log in to answer this question. Start asking to get answers Find the answer to your question by asking. Ask question Explore related questions pr.probability oc.optimization-and-control calculus-of-variations See similar questions with these tags. The Overflow Blog Now everyone can chat on Stack Overflow Featured on Meta A proposal for bringing back Community Promotion & Open Source Ads Community Asks Sprint Announcement – January 2026: Custom site-specific badges! Citation Helper v2 - User Script edition! Related 5 Characteristic polynomials of certain random symmetric matrices and the complexity of random Morse functions 0 Conditional Density of Random Variables 0 About the suboptimality of linear estimators 4 Optimal control of a robot 6 Rate of convergence of uniform order statistics to their expectations 6 Gradient of Wasserstein distance in the sense of Otto's calculus 2 How to solve an optimization problem whose optimization variable is a function? $(function() { $(\".js-gps-related-questions .spacer\").on(\"click\", function () { fireRelatedEvent($(this).index() + 1, $(this).data('question-id')); }); function fireRelatedEvent(position, questionId) { StackExchange.using(\"gps\", function() { StackExchange.gps.track('related_questions.click', { position: position, originQuestionId: 290854, relatedQuestionId: +questionId, location: 'sidebar', source: 'Baseline' }); }); } }); Question feed",
  "tags": "pr.probability,oc.optimization-and-control,calculus-of-variations,pr.probability,oc.optimization-and-control,calculus-of-variations",
  "scraped_at": "2026-01-14T04:16:08.423684",
  "answer": ""
}
{
  "id": "q_365789",
  "mathoverflow_id": 365789,
  "url": "https://mathoverflow.net/questions/365789/entropy-of-markov-measure-using-marginal-distribution",
  "title": "ds.dynamical systems - Entropy of Markov measure using marginal distribution",
  "body": "Let $S$ be a finite set and let $\\mathcal{X}$ be the set of all bi-infinite sequences over $S$ . Let $\\eta_1,\\eta_2$ be two shift invariant 1-step Markov measures over $\\mathcal{X}$ . For a finite word $w$ over $S$ we denote by $C_w$ the cylinder set defined by $w$ . Assume that $$-\\sum_{w\\in S^2} \\eta_1 (C_w)\\log \\eta_1 (C_w)\\geq -\\sum_{w\\in S^2} \\eta_2 (C_w)\\log \\eta_2 (C_w).$$ Does this imply that $h(\\eta_1)\\geq h(\\eta_2)$ where $h(\\cdot)$ is the entropy function? Is there an example for two different translation-invariant Markov measures with different entropies but with the same distribution of pairs? Thanks ds.dynamical-systems stochastic-processes entropy Share Cite Improve this question Follow edited Jul 16, 2020 at 18:25 asked Jul 16, 2020 at 15:31 NickOstr 11 2 2 bronze badges $\\endgroup$ 5 $\\begingroup$ What do you mean by Markov? The probability of putting a symbol at 0 depends only on the symbol at position $-1$? Or could it depend on a finite number of symbols? (In that case there are trivial counterexamples). $\\endgroup$ Anthony Quas –  Anthony Quas 2020-07-16 16:50:03 +00:00 Commented Jul 16, 2020 at 16:50 $\\begingroup$ Assuming we’re talking about 1-step Markov, the pair distribution determines the transition probability matrix, so the answer to your second question is no. The answer to your first question also has to be no: take two MCs with the same 2-block entropy. This certainly doesn’t imply that they have equal entropies. Now tweak the MC with higher entropy slightly to slightly reduce its 2-block entropy. $\\endgroup$ Anthony Quas –  Anthony Quas 2020-07-16 16:54:12 +00:00 Commented Jul 16, 2020 at 16:54 $\\begingroup$ @AnthonyQuas Thank you! Of course I meant 1-step Markov chain (corrected in the question). Regarding the second comment, this is somewhat confuses me. If the pair distribution determines the transition probability, and the entropy of a Markov chain is a function of that transition probability, doesn't it mean that higher entropy of pairs incurs higher entropy? $\\endgroup$ NickOstr –  NickOstr 2020-07-16 18:25:06 +00:00 Commented Jul 16, 2020 at 18:25 $\\begingroup$ Here's a trivial (degenerate) example. Let $S=\\{a,b,c,d,e,0,1\\}$. Let $\\eta_1$ be the measure on all constant sequences of letters: measure $\\frac 15$ on each of $\\bar a$, $\\bar b$, $\\bar c$, $\\bar d$, $\\bar e$. Let $\\eta_2$ be the i.i.d. $\\frac 12,\\frac 12$ measure on 0's and 1's. Then the two-block entropy of $\\eta_1$ is $\\log 5$, while that of $\\eta_2$ is $\\log 4$. But of course $h(\\eta_1)=0$ and $h(\\eta_2)=\\log 2$. You may not like the degenerate example, but the entropy is continuous, so you can make tiny perturbations and preserve the inequalities. $\\endgroup$ Anthony Quas –  Anthony Quas 2020-07-16 23:04:43 +00:00 Commented Jul 16, 2020 at 23:04 $\\begingroup$ @AnthonyQuas Thats actually a nice example. Got it, thanks! $\\endgroup$ NickOstr –  NickOstr 2020-07-17 12:11:43 +00:00 Commented Jul 17, 2020 at 12:11 Add a comment  |  0 Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) You must log in to answer this question. Start asking to get answers Find the answer to your question by asking. Ask question Explore related questions ds.dynamical-systems stochastic-processes entropy See similar questions with these tags. The Overflow Blog Now everyone can chat on Stack Overflow Featured on Meta A proposal for bringing back Community Promotion & Open Source Ads Community Asks Sprint Announcement – January 2026: Custom site-specific badges! Citation Helper v2 - User Script edition! Related 13 Entropy for Haar measure on $O(n)$ 4 Entropy inequality 5 Maximizing entropy under constraints 4 Measures maximizing entropy in a set of measures with fixed average for some observable 4 When entropy SRB measure is zero 8 Connection between entropy and the set of factors of a sequence $(function() { $(\".js-gps-related-questions .spacer\").on(\"click\", function () { fireRelatedEvent($(this).index() + 1, $(this).data('question-id')); }); function fireRelatedEvent(position, questionId) { StackExchange.using(\"gps\", function() { StackExchange.gps.track('related_questions.click', { position: position, originQuestionId: 365789, relatedQuestionId: +questionId, location: 'sidebar', source: 'Baseline' }); }); } }); Question feed",
  "tags": "ds.dynamical-systems,stochastic-processes,entropy,ds.dynamical-systems,stochastic-processes,entropy",
  "scraped_at": "2026-01-14T04:23:03.350503",
  "answer": ""
}
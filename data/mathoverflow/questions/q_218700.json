{
  "id": "q_218700",
  "mathoverflow_id": 218700,
  "url": "https://mathoverflow.net/questions/218700/summing-up-costs-over-a-markov-chain",
  "title": "pr.probability - Summing up costs over a Markov chain",
  "body": "I apologize in advance if this question is too simplistic to be appropriate for MathOverflow. I have inquired in multiple places but have found little to indicate that this is a previously studied problem, and my own understanding of probability and discrete mathematics is too sparse and incomplete for me to have any sort of intuition about whether this is a difficult problem or not. Consider a Markov chain with $N$ nodes and let $T_{ij}$ and $C_{ij}$ respectively denote the transition probability and 'cost' for moving from node $i$ to $j$. Now consider a position vector $p(t)$ which has $N$ entries; at time step $t$, the probability of being at node $i$ is given by $p_i(t)$. Consider also a cost vector $c(t)$ which has $N$ entries; at time step $t$, the cost incurred conditioned on being at node $j$ is given by $c_j(t)$. At time $t = 0$, we have probability $1$ of being on a specific 'starting' node. In our transition matrix $T$, we add an attractor at a 'final' node, setting the transition probability away from it to $0$. Now suppose that we iterate this model forward in time with the standard update $p(t+1) = p(t) \\cdot T$ on the position vector and $$ c_i(t+1) = \\sum_{j = 1}^N \\mathrm{P}(\\mathrm{was\\ at\\ }j | \\mathrm{am\\ at\\ }i) (c_j(t) + C_{ji}) $$ for the cost vector, where $$ \\mathrm{P}(\\mathrm{was\\ at\\ }j | \\mathrm{am\\ at\\ }i) = \\frac{\\mathrm{P}(\\mathrm{am\\ at\\ }i | \\mathrm{was\\ at\\ }j) \\cdot \\mathrm{P}(\\mathrm{was\\ at\\ }j)}{\\mathrm{P}(\\mathrm{am\\ at\\ }i)} = \\frac{T_{ji} \\cdot p_j(t)}{p_i(t+1)}. $$ Let $f$ be index of the final node. Moving the model forward in time until the probability of having reached the final node is above some certain threshold, e.g. $0.9999$, we obtain, at each time $t$, a point $(c_f(t), p_f(t))$ on the 'cumulative distribution curve of cost', i.e. a graph that relates the probability of having reached the final node given a particular amount of total expenditure. This is a workable computational way of plotting this 'cumulative distribution of cost'. However, I am curious if we can say take this forward analytically any further than we already have. I am not looking for any specific thing in particular, rather I simply want to know if we can say anything interesting about this 'cumulative distribution of cost' without resorting to brute force computations. The structures of $T$ and $C$ are quite arbitrary in practice, so generalized approaches are best, but I am also curious to know what could be done if certain limitations are imposed upon the structure of $T$ and $C$. Any pointers to books or papers would also be appreciated. pr.probability stochastic-processes markov-chains Share Cite Improve this question Follow edited May 23, 2020 at 4:41 Martin Sleziak 4,813 4 4 gold badges 39 39 silver badges 42 42 bronze badges asked Sep 19, 2015 at 13:48 Marcus Emilsson 101 3 3 bronze badges $\\endgroup$ 3 $\\begingroup$ Why not just simulate it using something like MC simulation? $\\endgroup$ Daniel Parry –  Daniel Parry 2015-09-19 15:35:08 +00:00 Commented Sep 19, 2015 at 15:35 $\\begingroup$ I haven't digested everything, but this sounds as though you should be able to write linear equations for what you want; then just solve and you're done. $\\endgroup$ Anthony Quas –  Anthony Quas 2015-09-19 17:52:09 +00:00 Commented Sep 19, 2015 at 17:52 $\\begingroup$ What do I miss? Let $S := \\{1,\\ldots,n\\}$ be the state space of nodes and $V_s$ be the expected total costs if we start in \"arbitrary\" starting node $s$. Let further $G_s := \\sum_{j \\in S} C_{sj}T_{sj}$ be the \"one-stage\" costs if we are in $s$. Then $V = G + TV$, hence $V = (E-T)^{-1}G$ with identity matrix $E$. We only have to ensure that $E-T$ is invertible, in particular we need that all $s \\not= f$ are transient. $\\endgroup$ Dieter Kadelka –  Dieter Kadelka 2020-05-23 20:01:00 +00:00 Commented May 23, 2020 at 20:01 Add a comment  |  0 Sorted by: Reset to default Highest score (default) Date modified (newest first) Date created (oldest first) You must log in to answer this question. Start asking to get answers Find the answer to your question by asking. Ask question Explore related questions pr.probability stochastic-processes markov-chains See similar questions with these tags. The Overflow Blog Now everyone can chat on Stack Overflow Featured on Meta A proposal for bringing back Community Promotion & Open Source Ads Community Asks Sprint Announcement – January 2026: Custom site-specific badges! Citation Helper v2 - User Script edition! Related 4 bound the hitting time in Markov chain 0 Estimates for the mixing time of a Markov Chain with biased initiation 2 Stationary distribution for time-inhomogeneous Markov process 3 Markov chain dichotomy 2 What conditions on the rate matrix $Q$ ensure unique convergence in continuous-time Markov chains? 0 Limit distribution of this discrete time Markov chain is standard normal? $(function() { $(\".js-gps-related-questions .spacer\").on(\"click\", function () { fireRelatedEvent($(this).index() + 1, $(this).data('question-id')); }); function fireRelatedEvent(position, questionId) { StackExchange.using(\"gps\", function() { StackExchange.gps.track('related_questions.click', { position: position, originQuestionId: 218700, relatedQuestionId: +questionId, location: 'sidebar', source: 'Baseline' }); }); } }); Question feed",
  "tags": "pr.probability,stochastic-processes,markov-chains,pr.probability,stochastic-processes,markov-chains",
  "scraped_at": "2026-01-14T04:13:16.179506",
  "answer": ""
}